canvas_deepseekocr/
├─ image_conversion.py
├─ merge_canvas.py
├─ integration.py     # (просто пример обёртки)
└─ prompt2canvas.py   # CLI-скрипт

минимальные пакеты:

pip install pillow pymupdf python-magic  # python-magic можно опустить, но лучше оставить
sudo apt install libmagic1

pillow — работа с изображениями

pymupdf — извлечение первой страницы PDF

python-magic — определение MIME-типа (помогает распознать случаи, когда расширение неверно)


Проверка запуска:

cd canvas_deepseekocr
python prompt2canvas.py "Hello, world!"

Появится файл canvas.png с текстом

с вложениями:

python prompt2canvas.py "Документы к заявке" spec.pdf photo.jpg archive.zip

Неподдерживаемые типы файлов выводят предупреждение, но не ломают скрипт.

Дальнейшие шаги.
Что происходит после того, как скрипт сформировал «канvas»-картинку из промта + вложений:

| № | Шаг пайплайна                     | Ключевые файлы/классы                      | Объяснение                                                                                                                                                        |
| - | --------------------------------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 | **Препроцессинг холста**          | `deepseek_ocr/utils/preprocess.py`         | холст приводится к нужному разрешению, разбивается на патчи, нормализуется под vision-энкодер                                                                             |
| 2 | **OCR-inference**                 | `deepseek_ocr/model/predict.py`            | картинка прогоняется через DeepSeek-OCR: на выходе ― JSON с текстом, bboxes, confidence                                                                                   |
| 3 | **Семантическая токенизация**     | `deepseek_ocr/utils/semantic_tokenizer.py` | извлечённый текст режется на «семантические чанки», кодируется в компактные int-токены                                                                                    |
| 4 | **Сжатие в HLLSet**               | `deepseek_ocr/hllset/hllset.py`            | набор токенов складывается в HyperLogLog-структуру (≈ битовый «отпечаток» содержания)                                                                                     |
| 5 | **Запись в Git-Cortex**           | `git_cortex.py`, класс `GitBackedCortex`   | HLLSet → Git-blob, Контекст → Git-tree, Связи → Git-commit ― всё версионируется; можно мерджить, откатывать и т.д. ([GitHub][1])                                          |
| 6 | **Граф SGS / Reasoning** *(опц.)* | `SGS_graph_roaring_bitmap.md`              | несколько HLLSet-узлов объединяются в Roaring-Bitmap-граф для быстрого поиска сходства                                                                                    |
| 7 | **Ответ LLM-агента**              | `integration.py` (пример)                  | агент может: ① вытянуть релевантные контексты из Git-Cortex; ② сформировать JSON-prompt; ③ передать его любой LLM (DeepSeek, Qwen, пр.) для итогового ответа пользователю |


Контент-адресность: картинка и извлечённый текст кодируются в HLLSet, который уникален при любом повторном появлении (даже в другом контексте).
Версионирование/происхождение: Git хранит кто добавил кусок, когда и с каким состоянием — удобно для аудита и коллаборации.
Быстрый поиск: Roaring-bitmap на основе HLLSet делает «приближённый индекс» — можно мгновенно найти все промты, где встречался, скажем, "Lat 50.45 Lon 30.52" или неявно похожие документы.
Унифицированный ввод для LLM: вместо «сырых» PDF/ZIP/картинок LLM получает структурированный текст плюс набор ссылок на Git-объекты — меньше токенов, выше контекст-window-эффективность.

---------------------------------------------------------------------------------------------------------------------
Дальнейшие шаги:

1. Проверить все шаги пайплайна

2. Посмотреть примеры
В examples/ есть скрипты вроде pipeline_hllset_git.py, весь процесс вроде бы свёрнут в 10-15 строк (надо посмотреть как Git-объекты появляются).

3. Собрать собственный индекс
Запустить prompt2canvas.py на пачке диалогов → получить HLLSets → складывать в Git-Cortex. После этого можно попробовать искать «похожие промты».

4. Подключить LLM
В примере integration.py показано, как вытаскивать ближайшие контексты и составлять system-prompt для ChatGPT/DeepSeek-LLM, чтобы ответы шли «с опорой на документы».




