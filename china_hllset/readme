| Цель                                               | Что внутри                                                                                                                             |
| -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Словарь 1 000–1 200 самых частотных иероглифов** | Берём частотный список (1 000+) и вытягиваем для каждого иероглифа его толкование из CC-CEDICT. ([Wiktionary][1])                      |
| **HLLSet для каждого иероглифа**                   | HyperLogLog-эскиз, в который добавлены *хэши* всех токенов, встретившихся в толковании (и, при желании, самих «связанных» иероглифов). |
| **Граф «связей»**                                  | Похожесть = пересечение двух HLL-эскизов; если Jaccard ≥ порога → ребро.                                                               |
| **Инвертированный индекс «символ → hash → HLL»**   | Позволяет по хэшу токена быстро вернуть, в каких иероглифах он встречается, и сразу объединить их HLL-эскизы.                          |

[1]: https://en.wiktionary.org/wiki/Appendix%3AMandarin_Frequency_lists/1-1000?utm_source=chatgpt.com "Appendix:Mandarin Frequency lists/1-1000"

Цель скрипта build_hll_chars.py — скачать частотный список китайских иероглифов (≈ 1 200 шт.), 
взять их толкования из словаря CC-CEDICT, превратить каждое толкование в HLL-эскиз (HyperLogLog), 
построить граф похожести и сделать быстрый «обратный» индекс «токен → иероглифы».

ecnfyjdrf пакетов:

pip install datasketch tqdm requests lxml

datasketch — даёт HyperLogLog (HyperLogLog), MinHash и др.

tqdm — красивые прогресс-бары в консоли.

lxml — быстрая XML/*HTML парсилка; нам нужна, чтобы безопасно «распилить» строки CC-CEDICT.

requests — просто HTTP-клиент для скачивания файлов.

china_hllset/
├── build_hll_chars.py  # сам скрипт
└── data/               # скачанные и промежуточные файлы (создаётся автоматически)

Запуск:

python build_hll_chars.py

В консоли:

Downloading ...cedict... → data/cedict.txt.gz
Downloading ...freq...   → data/freq.txt
HLL sets: 100%|█████| 1200/1200 [00:02<00:00, 525 it/s]
Similarity: 100%|█████| 718k/718k [00:08<00:00, 85k it/s]
✓ char_hll_db.json ready


Время: ~10 с на обычном ноуте.

Память: < 300 МБ (можно снизить, уменьшив p или число символов).

Итоговый char_hll_db.json ≈ 3–4 МБ.

Что лежит в char_hll_db.json:

{
  "meta": { "hll_p": 14 },
  "characters": {
    "爱": {
      "cedict": "to love; affection; ...",
      "hll_hex": "006b0f..."           // 1.6 КБ hex
    },
    ...
  },
  "edges": [
    ["的","之",0.231],                // связь + оценка похожести
    ["他","她",0.193],
    ...
  ],
  "inv_index": {
    "ab37c1d9e8a6f4b0": ["大","太", ...],
    ...
  }
}

-------------------------------------------------------

Дальнейшие шаги с этим направлением:

1. Проверить похожесть двух символов, найти символы, где встречается слово, навпример «water», визуализировать граф (упрощённо)

2. Донормировать токены — стемминг, удаление стоп-слов, Pinyin и т.д.

3. Связи по графике — добавить радикалы и структуру иероглифа как токены?

4. Построить веб-демо — Flask + Vue/React: вводишь слово → видишь ближайшие иероглифы.

5. Инкрементальные апдейты — чтение char_hll_db.json, обновление конкретного символа и пересохранение, без полного пересчёта.

